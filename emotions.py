# -*- coding: utf-8 -*-
"""Emotions

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MK0qYDjo6DCzWcuNO0Mgx-XyNcrmVw_S
"""

import pandas as pd
import numpy as np

from google.colab import files
uploaded = files.upload()

data = pd.read_csv('emotion-labels-test.csv')

data.head()

new_column_names = {'label': 'Emotions'}
data = data.rename(columns=new_column_names)
display(data.head())

import keras
import tensorflow
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Flatten, Dense

#As this is a problem of nlp ,i'll start by tokenizing the data
text = data['text'].tolist()
labels = data['Emotions'].tolist()

#Tokenize the text data
tokenizer = Tokenizer()
tokenizer.fit_on_texts(text)

#Now we need to pad the sequences to the same length to feed them into a neural networks.
#Here's how we can part the sequences of the texts to have the same length.

sequences = tokenizer.texts_to_sequences(text) #Converts text into sequences of word indices using the tokenizer.
max_length = max(len(seq) for seq in sequences)#Finds the length of the longest sequence of word indices.
paded_sequences = pad_sequences(sequences, maxlen=max_length)#Pads all sequences with zeros to make them the same length as the longest one.

#Now i'll use the label encoder method to convert the class from strings to a numerical representation
label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(labels)#This code converts text labels like 'joy' or 'sadness' into numbers using a LabelEncoder so a machine learning model can process them

#one hot encode the labels
one_hot_labels = keras.utils.to_categorical(labels)#1 0 0,0 1 0, 0 0 1

"""**Text Emotions Classification Model**


"""

# Split the data into training and testing sets
xtrain, xtest, ytrain, ytest = train_test_split(paded_sequences,
                                                one_hot_labels,
                                                test_size=0.2)

#embeddings allow neural networks to work with text data in a way that captures meaning and relationships between words in a compact and efficient format, leading to better performance on NLP tasks.

#tokenizer.word_index would assign a number to each unique word, like this:
#"hello": 1,"world": 2,great": 3
#The number of unique words here is 3. So, len(tokenizer.word_index) is 3.

#When we say input_dim=len(tokenizer.word_index) + 1, it means input_dim = 3 + 1 = 4.

#Now lets define a neural network architecture for our classification problem and use it to train a model to classify emotions:
model = Sequential()
model.add(Embedding(input_dim=len(tokenizer.word_index) + 1,
                    output_dim=100,input_length=max_length))
model.add(Flatten())#The Flatten layer reshapes the input into a single, long vector.
model.add(Dense(units=128, activation='relu'))#This defines a layer with 128 neurons that use the ReLU activation function to introduce non-linearity.
model.add(Dense(units=len(one_hot_labels[0]), activation='softmax'))#Softmax converts output scores into probabilities that sum to 1 for multi-class predictio

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # Changed optimizer to adam and loss to categorical_crossentropy
model.fit(xtrain, ytrain, epochs=20, batch_size=32, validation_data=(xtest, ytest))

input_text = "She didn't come today because she lost her dog yestertay!"

# Preprocess the input text
input_sequence = tokenizer.texts_to_sequences([input_text])
padded_input_sequence = pad_sequences(input_sequence, maxlen=max_length)#Pads the numerical sequence to the fixed length expected by the model.
prediction = model.predict(padded_input_sequence)# Feeds the padded sequence into the trained model to get prediction scores.
predicted_label = label_encoder.inverse_transform([np.argmax(prediction[0])])# Finds the index of the highest score and converts it back to the original emotion label.
print(predicted_label)

input = "iam very happy to see you my dear"
input_sequence = tokenizer.texts_to_sequences([input])
padded_input_sequence = pad_sequences(input_sequence, maxlen=max_length)#
prediction = model.predict(padded_input_sequence)
predicted_label = label_encoder.inverse_transform([np.argmax(prediction[0])])# Finds the index of the highest score and converts it back to the original emotion label.
print(predicted_label)

